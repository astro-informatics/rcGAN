+ echo

+ echo

+ echo

+ echo

+ cd /share/gpu0/jjwhit/rcGAN
+ srun python -u train.py --mass_mapping_8 --exp-name mmgan_training_8 --num-gpus 4
/share/gpu0/jjwhit/rcGAN/fastmri/__init__.py:16: UserWarning: Could not retrieve fastmri version!
  warnings.warn("Could not retrieve fastmri version!")
[rank: 0] Global seed set to 0
Experiment Name: mmgan_training_8
Number of GPUs: 4
wandb: Currently logged in as: jess-j-whitney. Use `wandb login --relogin` to force relogin
wandb: WARNING Path /share/gpu0/jjwhit/mass_map/mm_models/wandb/wandb/ wasn't writable, using system temp directory.
wandb: WARNING Path /share/gpu0/jjwhit/mass_map/mm_models/wandb/wandb/ wasn't writable, using system temp directory
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /tmp/wandb/run-20230629_113809-c3l171ac
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mmgan_training_8
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jess-j-whitney/mass_mapping_project
wandb: üöÄ View run at https://wandb.ai/jess-j-whitney/mass_mapping_project/runs/c3l171ac
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[rank: 0] Global seed set to 0
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
[W socket.cpp:426] [c10d] The server socket cannot be initialized on [::]:21407 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [compute-gpu-0-3.local]:21407 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [compute-gpu-0-3.local]:21407 (errno: 97 - Address family not supported by protocol).
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4]

  | Name          | Type               | Params
-----------------------------------------------------
0 | generator     | UNetModel          | 195 M 
1 | discriminator | DiscriminatorModel | 11.0 M
-----------------------------------------------------
206 M     Trainable params
0         Non-trainable params
206 M     Total params
827.392   Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
Traceback (most recent call last):
  File "train.py", line 102, in <module>
    trainer.fit(model, dm)
  File "/home/jjwhit/.conda/envs/cGAN/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 608, in fit
    call._call_and_handle_interrupt(
  File "/home/jjwhit/.conda/envs/cGAN/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py", line 38, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/jjwhit/.conda/envs/cGAN/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 650, in _fit_impl
    self._run(model, ckpt_path=self.ckpt_path)
  File "/home/jjwhit/.conda/envs/cGAN/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1103, in _run
    results = self._run_stage()
  File "/home/jjwhit/.conda/envs/cGAN/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1182, in _run_stage
    self._run_train()
  File "/home/jjwhit/.conda/envs/cGAN/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1195, in _run_train
    self._run_sanity_check()
  File "/home/jjwhit/.conda/envs/cGAN/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1267, in _run_sanity_check
    val_loop.run()
  File "/home/jjwhit/.conda/envs/cGAN/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py", line 199, in run
    self.advance(*args, **kwargs)
  File "/home/jjwhit/.conda/envs/cGAN/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py", line 152, in advance
    dl_outputs = self.epoch_loop.run(self._data_fetcher, dl_max_batches, kwargs)
  File "/home/jjwhit/.conda/envs/cGAN/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py", line 199, in run
    self.advance(*args, **kwargs)
  File "/home/jjwhit/.conda/envs/cGAN/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py", line 137, in advance
    output = self._evaluation_step(**kwargs)
  File "/home/jjwhit/.conda/envs/cGAN/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py", line 234, in _evaluation_step
    output = self.trainer._call_strategy_hook(hook_name, *kwargs.values())
  File "/home/jjwhit/.conda/envs/cGAN/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1485, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/jjwhit/.conda/envs/cGAN/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp.py", line 359, in validation_step
    return self.model(*args, **kwargs)
  File "/home/jjwhit/.conda/envs/cGAN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jjwhit/.conda/envs/cGAN/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1040, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/home/jjwhit/.conda/envs/cGAN/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1000, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/home/jjwhit/.conda/envs/cGAN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jjwhit/.conda/envs/cGAN/lib/python3.8/site-packages/pytorch_lightning/overrides/base.py", line 110, in forward
    return self._forward_module.validation_step(*inputs, **kwargs)
  File "/share/gpu0/jjwhit/rcGAN/models/lightning/mmGAN.py", line 191, in validation_step
    gens[:, z, :, :, :] = self.forward(y) * std[:, None, None, None] + mean[:, None, None, None]
  File "/share/gpu0/jjwhit/rcGAN/models/lightning/mmGAN.py", line 102, in forward
    samples = self.generator(torch.cat([y, noise], dim=1))
  File "/home/jjwhit/.conda/envs/cGAN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/share/gpu0/jjwhit/rcGAN/models/archs/mass_map/generator.py", line 181, in forward
    output, skip_out = layer(output)
  File "/home/jjwhit/.conda/envs/cGAN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/share/gpu0/jjwhit/rcGAN/models/archs/mass_map/generator.py", line 66, in forward
    skip_out = self.res(out)  # self.activation(self.conv_2(out))
  File "/home/jjwhit/.conda/envs/cGAN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/share/gpu0/jjwhit/rcGAN/models/archs/mass_map/generator.py", line 28, in forward
    return self.conv_1x1(x) + self.conv_block(x)
  File "/home/jjwhit/.conda/envs/cGAN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jjwhit/.conda/envs/cGAN/lib/python3.8/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/jjwhit/.conda/envs/cGAN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jjwhit/.conda/envs/cGAN/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/jjwhit/.conda/envs/cGAN/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 39.59 GiB total capacity; 10.36 GiB already allocated; 1.96 GiB free; 10.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: \ 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: | 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: / 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: - 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: üöÄ View run mmgan_training_8 at: https://wandb.ai/jess-j-whitney/mass_mapping_project/runs/c3l171ac
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20230629_113809-c3l171ac/logs
Sanity Checking: 0it [00:00, ?it/s]Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]srun: error: compute-gpu-0-3: task 0: Exited with exit code 1

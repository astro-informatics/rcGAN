{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7fd3402-5307-4545-8f2b-96afb6e73b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/share/gpu0/jjwhit/rcGAN/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5754d92b-cfe6-4399-963d-4203541616ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/gpu0/jjwhit/rcGAN/fastmri/__init__.py:16: UserWarning: Could not retrieve fastmri version!\n",
      "  warnings.warn(\"Could not retrieve fastmri version!\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import yaml\n",
    "import types\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from data.lightning.MassMappingDataModule import MMDataModule\n",
    "from utils.parse_args import create_arg_parser\n",
    "from pytorch_lightning import seed_everything\n",
    "from models.lightning.mmGAN import mmGAN\n",
    "from utils.mri.math import tensor_to_complex_np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7204ad68-a858-44e0-ae5d-bf2cb3334ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_object(dct):\n",
    "    return types.SimpleNamespace(**dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e6282f5-ac84-45c2-911c-0c61e7d42f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model here\n",
    "test_plot_model = mmGAN.load_from_checkpoint('/share/gpu0/jjwhit/mass_map/mm_models/mmgan_debug/checkpoint_best.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "562bf321-69a3-4ff9-8216-158277681792",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/share/gpu0/jjwhit/rcGAN/configs/mass_map.yml', 'r') as f:\n",
    "    cfg = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    cfg = json.loads(json.dumps(cfg), object_hook=load_object)\n",
    "\n",
    "dm = MMDataModule(cfg)\n",
    "fig_count = 1\n",
    "dm.setup()\n",
    "test_loader = dm.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88b27229-22e6-4d11-bdc0-af0d1ac8e856",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mmGAN(\n",
       "  (generator): UNetModel(\n",
       "    (down_sample_layers): ModuleList(\n",
       "      (0): ConvDownBlock(\n",
       "        (conv_1): Conv2d(4, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (res): ResidualBlock(\n",
       "          (conv_block): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): PReLU(num_parameters=1)\n",
       "            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (5): PReLU(num_parameters=1)\n",
       "          )\n",
       "          (conv_1x1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (1): ConvDownBlock(\n",
       "        (conv_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (res): ResidualBlock(\n",
       "          (conv_block): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): PReLU(num_parameters=1)\n",
       "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (5): PReLU(num_parameters=1)\n",
       "          )\n",
       "          (conv_1x1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (2): ConvDownBlock(\n",
       "        (conv_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (res): ResidualBlock(\n",
       "          (conv_block): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): PReLU(num_parameters=1)\n",
       "            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (5): PReLU(num_parameters=1)\n",
       "          )\n",
       "          (conv_1x1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (3): ConvDownBlock(\n",
       "        (conv_1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (res): ResidualBlock(\n",
       "          (conv_block): Sequential(\n",
       "            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): PReLU(num_parameters=1)\n",
       "            (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (5): PReLU(num_parameters=1)\n",
       "          )\n",
       "          (conv_1x1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): PReLU(num_parameters=1)\n",
       "      )\n",
       "    )\n",
       "    (res_layer_1): Sequential(\n",
       "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): PReLU(num_parameters=1)\n",
       "      (3): ResidualBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): PReLU(num_parameters=1)\n",
       "        )\n",
       "        (conv_1x1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (4): ResidualBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): PReLU(num_parameters=1)\n",
       "        )\n",
       "        (conv_1x1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (5): ResidualBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): PReLU(num_parameters=1)\n",
       "        )\n",
       "        (conv_1x1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (6): ResidualBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): PReLU(num_parameters=1)\n",
       "        )\n",
       "        (conv_1x1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (7): ResidualBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): PReLU(num_parameters=1)\n",
       "        )\n",
       "        (conv_1x1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): PReLU(num_parameters=1)\n",
       "    )\n",
       "    (up_sample_layers): ModuleList(\n",
       "      (0): ConvUpBlock(\n",
       "        (conv_1): ConvTranspose2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): PReLU(num_parameters=1)\n",
       "        (layers): Sequential(\n",
       "          (0): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): ResidualBlock(\n",
       "            (conv_block): Sequential(\n",
       "              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): PReLU(num_parameters=1)\n",
       "              (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (5): PReLU(num_parameters=1)\n",
       "            )\n",
       "            (conv_1x1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ConvUpBlock(\n",
       "        (conv_1): ConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): PReLU(num_parameters=1)\n",
       "        (layers): Sequential(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): ResidualBlock(\n",
       "            (conv_block): Sequential(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): PReLU(num_parameters=1)\n",
       "              (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (5): PReLU(num_parameters=1)\n",
       "            )\n",
       "            (conv_1x1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): ConvUpBlock(\n",
       "        (conv_1): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): PReLU(num_parameters=1)\n",
       "        (layers): Sequential(\n",
       "          (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): ResidualBlock(\n",
       "            (conv_block): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): PReLU(num_parameters=1)\n",
       "              (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (5): PReLU(num_parameters=1)\n",
       "            )\n",
       "            (conv_1x1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): ConvUpBlock(\n",
       "        (conv_1): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): PReLU(num_parameters=1)\n",
       "        (layers): Sequential(\n",
       "          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): ResidualBlock(\n",
       "            (conv_block): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): PReLU(num_parameters=1)\n",
       "              (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (5): PReLU(num_parameters=1)\n",
       "            )\n",
       "            (conv_1x1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (discriminator): DiscriminatorModel(\n",
       "    (initial_layers): Sequential(\n",
       "      (0): Conv2d(4, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (encoder_layers): ModuleList(\n",
       "      (0): AvgPool(in_chans=32, out_chans=64\n",
       "      ResBlock(in_chans=64, out_chans=64\n",
       "      (1): AvgPool(in_chans=64, out_chans=128\n",
       "      ResBlock(in_chans=128, out_chans=128\n",
       "      (2): AvgPool(in_chans=128, out_chans=256\n",
       "      ResBlock(in_chans=256, out_chans=256\n",
       "      (3): AvgPool(in_chans=256, out_chans=512\n",
       "      ResBlock(in_chans=512, out_chans=512\n",
       "      (4): AvgPool(in_chans=512, out_chans=512\n",
       "      ResBlock(in_chans=512, out_chans=512\n",
       "      (5): AvgPool(in_chans=512, out_chans=512\n",
       "      ResBlock(in_chans=512, out_chans=512\n",
       "      (6): AvgPool(in_chans=512, out_chans=512\n",
       "      ResBlock(in_chans=512, out_chans=512\n",
       "      (7): AvgPool(in_chans=512, out_chans=512\n",
       "      ResBlock(in_chans=512, out_chans=512\n",
       "    )\n",
       "    (dense): Sequential(\n",
       "      (0): Flatten(start_dim=1, end_dim=-1)\n",
       "      (1): Linear(in_features=8192, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_plot_model.cuda()\n",
    "test_plot_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "899af73f-e6cb-49a5-8f33-9e75331a355f",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 79.10 GiB total capacity; 73.68 GiB already allocated; 1.82 GiB free; 75.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m gens_rcgan \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m      9\u001b[0m     size\u001b[38;5;241m=\u001b[39m(y\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), cfg\u001b[38;5;241m.\u001b[39mnum_z_test, cfg\u001b[38;5;241m.\u001b[39min_chans \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, cfg\u001b[38;5;241m.\u001b[39mim_size, cfg\u001b[38;5;241m.\u001b[39mim_size, \u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m z \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg\u001b[38;5;241m.\u001b[39mnum_z_test):\n\u001b[0;32m---> 12\u001b[0m     gens_rcgan[:, z, :, :, :, :] \u001b[38;5;241m=\u001b[39m test_plot_model\u001b[38;5;241m.\u001b[39mreformat(\u001b[43mtest_plot_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     14\u001b[0m avg_rcgan \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(gens_rcgan, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m gt \u001b[38;5;241m=\u001b[39m test_plot_model\u001b[38;5;241m.\u001b[39mreformat(x)\n",
      "File \u001b[0;32m/share/gpu0/jjwhit/rcGAN/models/lightning/mmGAN.py:102\u001b[0m, in \u001b[0;36mmmGAN.forward\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    100\u001b[0m num_vectors \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    101\u001b[0m noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_noise(num_vectors)\n\u001b[0;32m--> 102\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreadd_measures(samples, y)     \n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples\n",
      "File \u001b[0;32m~/.conda/envs/cGAN/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/share/gpu0/jjwhit/rcGAN/models/archs/mass_map/generator.py:188\u001b[0m, in \u001b[0;36mUNetModel.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# Apply up-sampling layers\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_sample_layers:\n\u001b[0;32m--> 188\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(output)\n",
      "File \u001b[0;32m~/.conda/envs/cGAN/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/share/gpu0/jjwhit/rcGAN/models/archs/mass_map/generator.py:108\u001b[0m, in \u001b[0;36mConvUpBlock.forward\u001b[0;34m(self, input, skip_input)\u001b[0m\n\u001b[1;32m    105\u001b[0m upsampled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_1(\u001b[38;5;28minput\u001b[39m, output_size\u001b[38;5;241m=\u001b[39mresidual_skip\u001b[38;5;241m.\u001b[39msize())))\n\u001b[1;32m    106\u001b[0m concat_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([residual_skip, upsampled], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcat_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cGAN/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/cGAN/lib/python3.8/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/cGAN/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/share/gpu0/jjwhit/rcGAN/models/archs/mass_map/generator.py:28\u001b[0m, in \u001b[0;36mResidualBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_1x1(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cGAN/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/cGAN/lib/python3.8/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/cGAN/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/cGAN/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cGAN/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 79.10 GiB total capacity; 73.68 GiB already allocated; 1.82 GiB free; 75.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(test_loader):\n",
    "    y, x, mean, std = data\n",
    "    y = y.cuda()\n",
    "    x = x.cuda()\n",
    "    mean = mean.cuda()\n",
    "    std = std.cuda()\n",
    "\n",
    "    gens_rcgan = torch.zeros(\n",
    "        size=(y.size(0), cfg.num_z_test, cfg.in_chans // 2, cfg.im_size, cfg.im_size, 2)).cuda()\n",
    "    \n",
    "    for z in range(cfg.num_z_test):\n",
    "        gens_rcgan[:, z, :, :, :, :] = test_plot_model.reformat(test_plot_model.forward(y))\n",
    "\n",
    "    avg_rcgan = torch.mean(gens_rcgan, dim=1)\n",
    "\n",
    "    gt = test_plot_model.reformat(x)\n",
    "    zfr = test_plot_model.reformat(y)\n",
    "\n",
    "    for j in range(y.size(0)):\n",
    "        np_avgs = {\n",
    "            'rcgan': None,\n",
    "        }\n",
    "\n",
    "        np_samps = {\n",
    "            'rcgan': [],\n",
    "        }\n",
    "\n",
    "        np_stds = {\n",
    "            'rcgan': None,\n",
    "        }\n",
    "\n",
    "        np_gt = None\n",
    "\n",
    "\n",
    "        np_gt = ndimage.rotate(\n",
    "            torch.tensor(tensor_to_complex_np((gt[j] * std[j] + mean[j]).cpu())).abs().numpy(), 180) #What is the 180 for?\n",
    "        np_zfr = ndimage.rotate(\n",
    "            torch.tensor(tensor_to_complex_np((zfr[j] * std[j] + mean[j]).cpu())).abs().numpy(), 180)\n",
    "\n",
    "        np_avgs['rcgan'] = ndimage.rotate(\n",
    "            torch.tensor(tensor_to_complex_np((avg_rcgan[j] * std[j] + mean[j]).cpu())).abs().numpy(),\n",
    "            180)\n",
    "\n",
    "        for z in range(cfg.num_z_test):\n",
    "            np_samps['rcgan'].append(ndimage.rotate(torch.tensor(\n",
    "                tensor_to_complex_np((gens_rcgan[j, z] * std[j] + mean[j]).cpu())).abs().numpy(), 180))\n",
    "\n",
    "        np_stds['rcgan'] = np.std(np.stack(np_samps['rcgan']), axis=0)\n",
    "\n",
    "        method = 'rcgan'\n",
    "        zoom_startx = np.random.randint(120, 250) #What is this section for?\n",
    "        zoom_starty1 = np.random.randint(30, 80)\n",
    "        zoom_starty2 = np.random.randint(260, 300)\n",
    "\n",
    "        p = np.random.rand()\n",
    "        zoom_starty = zoom_starty1\n",
    "        if p <= 0.5:\n",
    "            zoom_starty = zoom_starty2\n",
    "\n",
    "        zoom_length = 80\n",
    "\n",
    "        x_coord = zoom_startx + zoom_length\n",
    "        y_coords = [zoom_starty, zoom_starty + zoom_length]\n",
    "\n",
    "        # Global recon, error, std\n",
    "        nrow = 1\n",
    "        ncol = 4\n",
    "\n",
    "        fig = plt.figure(figsize=(ncol + 1, nrow + 1))\n",
    "\n",
    "        gs = gridspec.GridSpec(nrow, ncol,\n",
    "                               wspace=0.0, hspace=0.0,\n",
    "                               top=1. - 0.5 / (nrow + 1), bottom=0.5 / (nrow + 1),\n",
    "                               left=0.5 / (ncol + 1), right=1 - 0.5 / (ncol + 1))\n",
    "\n",
    "        ax = plt.subplot(gs[0, 0])\n",
    "        ax.imshow(np_gt, cmap='gray', vmin=0, vmax=0.7 * np.max(np_gt))\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(\"Truth\")\n",
    "\n",
    "        ax = plt.subplot(gs[0, 1])\n",
    "        ax.imshow(np_avgs[method], cmap='gray', vmin=0, vmax=0.7 * np.max(np_gt))\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(method)\n",
    "\n",
    "\n",
    "        ax = plt.subplot(gs[0, 2])\n",
    "        im = ax.imshow(2 * np.abs(np_avgs[method] - np_gt), cmap='jet', vmin=0,\n",
    "                       vmax=np.max(np.abs(np_avgs['rcgan'] - np_gt)))\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(\"Error\")\n",
    "\n",
    "\n",
    "        ax = plt.subplot(gs[0, 3])\n",
    "        ax.imshow(np_stds[method], cmap='viridis', vmin=0, vmax=np.max(np_stds['rcgan']))\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(\"Std. Dev.\")\n",
    "\n",
    "        plt.savefig(f'/share/gpu0/jjwhit/test_figures/test_fig_avg_err_std_{fig_count}.png', bbox_inches='tight', dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "        nrow = 1\n",
    "        ncol = 8\n",
    "\n",
    "        fig = plt.figure(figsize=(ncol + 1, nrow + 1))\n",
    "\n",
    "        gs = gridspec.GridSpec(nrow, ncol,\n",
    "                               wspace=0.0, hspace=0.0,\n",
    "                               top=1. - 0.5 / (nrow + 1), bottom=0.5 / (nrow + 1),\n",
    "                               left=0.5 / (ncol + 1), right=1 - 0.5 / (ncol + 1))\n",
    "\n",
    "        ax = plt.subplot(gs[0, 0])\n",
    "        ax.imshow(np_gt, cmap='gray', vmin=0, vmax=0.7 * np.max(np_gt))\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title('Truth')\n",
    "\n",
    "        ax1 = ax\n",
    "\n",
    "        rect = patches.Rectangle((zoom_startx, zoom_starty), zoom_length, zoom_length, linewidth=1,\n",
    "                                 edgecolor='r',\n",
    "                                 facecolor='none')\n",
    "\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        ax = plt.subplot(gs[0, 1])\n",
    "        ax.imshow(np_gt[zoom_starty:zoom_starty + zoom_length, zoom_startx:zoom_startx + zoom_length],\n",
    "                  cmap='gray',\n",
    "                  vmin=0, vmax=0.7 * np.max(np_gt))\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title('Truth')\n",
    "\n",
    "        connection_path_1 = patches.ConnectionPatch([zoom_startx + zoom_length, zoom_starty],\n",
    "                                                    [0, 0], coordsA=ax1.transData,\n",
    "                                                    coordsB=ax.transData, color='r')\n",
    "        fig.add_artist(connection_path_1)\n",
    "        connection_path_2 = patches.ConnectionPatch([zoom_startx + zoom_length, zoom_starty + zoom_length], [0, zoom_length],\n",
    "                                                    coordsA=ax1.transData,\n",
    "                                                    coordsB=ax.transData, color='r')\n",
    "        fig.add_artist(connection_path_2)\n",
    "\n",
    "        ax = plt.subplot(gs[0, 2])\n",
    "        ax.imshow(\n",
    "            np_avgs[method][zoom_starty:zoom_starty + zoom_length, zoom_startx:zoom_startx + zoom_length],\n",
    "            cmap='gray', vmin=0, vmax=0.7 * np.max(np_gt))\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title('32-Avg.')\n",
    "\n",
    "        ax = plt.subplot(gs[0, 3])\n",
    "        avg = np.zeros((384, 384))\n",
    "        for l in range(4):\n",
    "            avg += np_samps[method][l]\n",
    "\n",
    "        avg = avg / 4\n",
    "        \n",
    "        ax.imshow(\n",
    "            avg[zoom_starty:zoom_starty + zoom_length, zoom_startx:zoom_startx + zoom_length],\n",
    "            cmap='gray', vmin=0, vmax=0.7 * np.max(np_gt))\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title('4-Avg.')\n",
    "\n",
    "        ax = plt.subplot(gs[0, 4])\n",
    "        avg = np.zeros((384, 384))\n",
    "        for l in range(2):\n",
    "            avg += np_samps[method][l]\n",
    "\n",
    "        avg = avg / 2\n",
    "        ax.imshow(\n",
    "            avg[zoom_starty:zoom_starty + zoom_length, zoom_startx:zoom_startx + zoom_length],\n",
    "            cmap='gray', vmin=0, vmax=0.7 * np.max(np_gt))\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title('2-Avg.')\n",
    "\n",
    "        for samp in range(2):\n",
    "            ax = plt.subplot(gs[0, samp + 5])\n",
    "            ax.imshow(np_samps[method][samp][zoom_starty:zoom_starty + zoom_length,\n",
    "                      zoom_startx:zoom_startx + zoom_length], cmap='gray', vmin=0,\n",
    "                      vmax=0.7 * np.max(np_gt))\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_title(f'Sample {samp + 1}')\n",
    "\n",
    "\n",
    "        ax = plt.subplot(gs[0, 7])\n",
    "        ax.imshow(np_stds[method][zoom_starty:zoom_starty + zoom_length,\n",
    "                  zoom_startx:zoom_startx + zoom_length], cmap='viridis', vmin=0,\n",
    "                  vmax=np.max(np_stds['rcgan']))\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title('Std. Dev.')\n",
    "\n",
    "        plt.savefig(f'/share/gpu0/jjwhit/test_figures/zoomed_avg_samps_{fig_count}.png', bbox_inches='tight', dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "        if fig_count == args.num_figs:\n",
    "            exit()\n",
    "        fig_count += 1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cGAN",
   "language": "python",
   "name": "cgan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
